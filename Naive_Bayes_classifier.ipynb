{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive_Bayes_classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fknjkD8gfQr5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#                            **Bernoulli Naive Bayes Classifier from scratch ** \n",
        ">>>> ***Spam/Ham classification***\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "l3JwGyvgge3s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This project consists of classifying whether a given message is a spam or a ham.\n",
        "This is possible with a function given by sklearn but the goal is to achieve the same result with a function made from scratch in order to understand the mathematical approach."
      ]
    },
    {
      "metadata": {
        "id": "mmgjIPnvgy0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To solve this problem, we first construct a dictionary and generate a feature vector that corresponds to the given message. \n",
        "The dictionary is constructed by listing all the words contained in a collection of messages. \n",
        "Then, we identify the words contained in a given message by generating the feature vector.\n",
        "\n",
        "\n",
        "**$\\begin{aligned} P \\left( x _ { 1 } , \\cdots , x _ { 500000 } | y \\right) & = P \\left( x _ { 1 } | y \\right) P \\left( x _ { 2 } | y , x _ { 1 } \\right) P \\left( x _ { 3 } | y , x _ { 1 } , x _ { 2 } \\right) \\cdots \\\\ & = P \\left( x _ { 1 } | y \\right) P \\left( x _ { 2 } | y \\right) P \\left( x _ { 3 } | y \\right) \\cdots P \\left( x _ { 50000 } | y \\right) \\\\ & = \\prod _ { n = 1 } ^ { N } P \\left( x _ { n } | y \\right) \\end{aligned}$**\n",
        "\n",
        "The above expression says that once we know that a message is Spam (or not), the presence of some words in this message does not help to know whether some other words are also present in that message.\n",
        "This (naive) assumption is obviously false. But, despite this incongruence, the Naive Bayes algorithm works well for classifying texts. On the other hand, without this assumption the problem to be solved would be much harder.\n"
      ]
    },
    {
      "metadata": {
        "id": "S42EAOnin_U1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Mathematical formulas : **\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Joint likelyhood : \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$\\mathcal { L } \\left( \\phi _ { y } , \\phi _ { j | y = 1 } , \\phi _ { j | y = 0 } \\right) = \\prod _ { i = 1 } ^ { I } P \\left( x ^ { ( i ) } , y ^ { ( i ) } \\right)$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* Maximum Likelihood Estimation (MLE) :\n",
        "\n",
        "\n",
        "\n",
        "$\\phi _ { y } = \\frac { 1 } { I } \\sum _ { i = 1 } ^ { I } \\mathbb { I } \\left\\{ y ^ { ( i ) } = 1 \\right\\}$\n",
        "\n",
        "$\\phi _ { n | y = 1 } = \\frac { \\sum _ { i = 1 } ^ { I } \\mathbb { I } \\left\\{ x _ { n } ^ { ( i ) } = 1 , y ^ { ( i ) } = 1 \\right\\} } { \\sum _ { i = 1 } ^ { I } \\mathbb { I } \\left\\{ y ^ { ( i ) } = 1 \\right\\} }$\n",
        "\n",
        "$\\phi _ { n | y = 0 } = \\frac { \\sum _ { i = 1 } ^ { I } \\mathbb { I } \\left\\{ x _ { n } ^ { ( i ) } = 1 , y ^ { ( i ) } = 0 \\right\\} } { \\sum _ { i = 1 } ^ { I } \\mathbb { I } \\left\\{ y ^ { ( i ) } = 0 \\right\\} }$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* Predict criterion : \n",
        "\n",
        "$\\underset { y } { \\arg \\max } P ( y | x ) = \\underset { y } { \\arg \\max } \\frac { P ( x | y ) P ( y ) } { P ( x ) } = \\underset { y } { \\arg \\max } P ( x | y ) P ( y )$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Bayes rule : **\n",
        "\n",
        "$P ( A | B ) = \\frac { P ( B | A ) P ( A ) } { P ( B ) }$\n"
      ]
    },
    {
      "metadata": {
        "id": "hqxyLVPUwCoX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing the needed libraries : **"
      ]
    },
    {
      "metadata": {
        "id": "fHc0g_WmwOZz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os,sys\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BpyomsfKdIC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "    \"\"\"\n",
        "    Read data 'line by line', using generators.\n",
        "    Generators make it easier to process BIG text files.\n",
        "    \"\"\"\n",
        "    with open(filename, 'r',encoding=\"utf8\", errors='ignore') as input:\n",
        "        for line in input:\n",
        "            yield line\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ykNmwvM-snLv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ham_split(data):\n",
        "  for i in data:\n",
        "    i=i.split(\"\\t\")\n",
        "    if i[0]=='ham':\n",
        "      yield i[1]\n",
        "\n",
        "def spam_split(data):\n",
        "  for i in data:\n",
        "    i=i.split(\"\\t\")\n",
        "    if i[0]=='spam':\n",
        "      yield i[1]\n",
        "\n",
        "def get_target(data) : \n",
        "  liste=[]\n",
        "  for i in data: \n",
        "    i=i.split(\"\\t\")\n",
        "    if i[0]=='ham' :\n",
        "      liste.append(0)\n",
        "    else :\n",
        "      liste.append(1)\n",
        "  return liste"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hjbd15NstpRc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Reading the data**"
      ]
    },
    {
      "metadata": {
        "id": "gC6-vgWSfwXY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_file=\"messages.txt\"\n",
        "\n",
        "#Accessing the data file\n",
        "data = read_data(data_file)\n",
        "data_list=list(data)\n",
        "\n",
        "#Splitting the data into training and test\n",
        "#Ratio used : 70%-30%\n",
        "size = len(data_list)\n",
        "training_size=int(0.7*size)\n",
        "test_size=int(0.3*size)\n",
        "\n",
        "training_data=data_list[:training_size]\n",
        "test_data=data_list[training_size:training_size+test_size]\n",
        "\n",
        "\n",
        "#Extracting the messages into list of hams and list of spams\n",
        "\n",
        "ham_training=list(ham_split(training_data))\n",
        "ham_test=list(ham_split(test_data))\n",
        "spam_training=list(spam_split(training_data))\n",
        "spam_test=list(spam_split(test_data))\n",
        "\n",
        "#Associated target lists : \n",
        "# Ham : 0\n",
        "# Spam : 1\n",
        "\n",
        "training_target_matrix=get_target(training_data)\n",
        "test_target_matrix=get_target(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "THP8CUvPt32b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Creating a dictionary**"
      ]
    },
    {
      "metadata": {
        "id": "T8XXGB9ieBNl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Make a dictionary based on message's words, return the most common 3000 words\n",
        "def make_Dictionary(data):\n",
        "    all_words = []\n",
        "    for line in data:\n",
        "      words = line.split()\n",
        "      all_words += words\n",
        "    for w in all_words: \n",
        "        if w.isalpha() == False:\n",
        "          for i in range(all_words.count(w)):\n",
        "            all_words.remove(w)\n",
        "        elif len(w) == 1:\n",
        "          for i in range(all_words.count(w)):\n",
        "            all_words.remove(w)\n",
        "    all_words=[x.lower() for x in all_words]\n",
        "    dictionary = Counter(all_words)\n",
        "    dictionary = dictionary.most_common(3000)\n",
        "    return dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I9kyvz9LgOeU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating dictionary based on the training data\n",
        "clean_training_data=ham_training+spam_training\n",
        "clean_test_data=ham_test+spam_test\n",
        "\n",
        "training_dictionary=make_Dictionary(clean_training_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-N9CSIs4utPA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Modeling the data **"
      ]
    },
    {
      "metadata": {
        "id": "GknN2AUxjHc_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create a matrix with the occurences of each features for every messages, based on the dictionary\n",
        "def extract_features(data,dictionary):\n",
        "    #files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]\n",
        "    features_matrix = np.zeros((len(data), 3000))\n",
        "    lineID = 0\n",
        "    for line in data:\n",
        "      words = line.split()\n",
        "      for word in words:\n",
        "        wordID = 0\n",
        "        for i, d in enumerate(dictionary):\n",
        "          if d[0] == word:\n",
        "            wordID = i\n",
        "            features_matrix[lineID, wordID] = 1 #Specific to Bernoulli\n",
        "      lineID = lineID + 1\n",
        "    return features_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fa5UitEKmsIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "ce2f8912-7adb-4afd-eedf-8c4ffcc7d67d"
      },
      "cell_type": "code",
      "source": [
        "#Extracting features from the training data and the test data\n",
        "train_matrix = extract_features(clean_training_data,training_dictionary)\n",
        "test_matrix = extract_features(clean_test_data,training_dictionary)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5e4f76adf358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_training_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_test_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f2b16d70cf3c>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(data, dictionary)\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mwordID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mwordID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "fMmPN1r6e_Ej",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit(matrix_input, matrix_target) :\n",
        "  true_probability = matrix_target.count(1)/len(matrix_target)\n",
        "  false_probability = 1 - true_probability\n",
        "  matrix_prob_positive = []\n",
        "  matrix_prob_negative = []\n",
        "  ham_count=0\n",
        "  spam_count=0\n",
        "  columns = [l for l in zip(*matrix_input)]\n",
        "  for i in range(len(columns)): #i->columns\n",
        "    for j in range(len(columns[i])): #j->rows\n",
        "      if matrix_target[j]==1:\n",
        "        spam_count=spam_count+columns[i][j]\n",
        "      else :\n",
        "        ham_count=ham_count+columns[i][j]\n",
        "    matrix_prob_positive.append((spam_count*true_probability)/matrix_target.count(1))\n",
        "    matrix_prob_negative.append((ham_count*false_probability)/matrix_target.count(0))\n",
        "    ham_count=0\n",
        "    spam_count=0\n",
        "  matrix_prob=[]\n",
        "  matrix_prob.append(matrix_prob_positive)\n",
        "  matrix_prob.append(matrix_prob_negative)\n",
        "  return matrix_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTTkkpwC-QDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(matrix_prob,matrix_test) :\n",
        "  result=[]\n",
        "  for i in range(len(matrix_test)):\n",
        "    pos_prob=1\n",
        "    neg_prob=1\n",
        "    for j in range(len(matrix_test[i])):\n",
        "      if matrix_test[i][j]==1:\n",
        "        pos_prob=pos_prob*(matrix_test[i][j]*matrix_prob[0][j])\n",
        "        neg_prob=neg_prob*(matrix_test[i][j]*matrix_prob[1][j])\n",
        "    decision = np.argmax([neg_prob,pos_prob])\n",
        "    if decision == 0 :\n",
        "      result.append(0)\n",
        "    elif decision == 1:\n",
        "      result.append(1)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z-9eZF25d3w-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Comparing results from the sklearn function and the created function**"
      ]
    },
    {
      "metadata": {
        "id": "LZqF69xJeOJ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### sklearn function : "
      ]
    },
    {
      "metadata": {
        "id": "cbmEI-q5KcKV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "model = BernoulliNB()\n",
        "model.fit(train_matrix, training_target_matrix)\n",
        "sklearn_result = model.predict(test_matrix)\n",
        "sklearn_cm=confusion_matrix(test_target_matrix, sklearn_result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZeogOWwvejgv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **created function :**"
      ]
    },
    {
      "metadata": {
        "id": "kD52oeUVeuCk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result=predict(fit(train_matrix, training_target_matrix),test_matrix)\n",
        "cm=confusion_matrix(test_target_matrix, result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BiIFOFxhe1aK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Comparing : "
      ]
    },
    {
      "metadata": {
        "id": "rcyYyFhXe9W8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sklearn_score =(sklearn_cm[0,0]+sklearn_cm[1,1])/(sklearn_cm[0,0]+sklearn_cm[0,1]+sklearn_cm[1,0]+sklearn_cm[1,1])\n",
        "score = (cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n",
        "\n",
        "print(\"sklearn score : \", round(sklearn_score*100, 2),\"%\")\n",
        "print(\"created function score : \", round(score*100, 2),\"%\")\n",
        "print(\"Accuracy : \", round( ((sklearn_score/score)*100) ,2),\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}